# -*- coding: utf-8 -*-
"""b22ee057_b22ee094.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HpK_8F_E2gBPlqChRTwm82kYurciNBZU

# Loading the MNIST Dataset from OpenML
"""

import numpy as np
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt
import time

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize the images by scaling the pixel values to the range [0, 1]
X_train = X_train / 255.0
X_test = X_test / 255.0

# Reshape the images to (number_of_samples, 28*28)
X_train = X_train.reshape(X_train.shape[0], 28 * 28)
X_test = X_test.reshape(X_test.shape[0], 28 * 28)

# One-hot encode the labels (10 classes)
def one_hot_encode(y, num_classes=10):
    return np.eye(num_classes)[y]

y_train = one_hot_encode(y_train)
y_test = one_hot_encode(y_test)

# Function to show images along with their sizes
def show_images_with_size(image, title, num_row=2, num_col=11):
    image_size = 28  # Image size is 28x28 pixels
    image = np.reshape(image, (image.shape[0], image_size, image_size))  # Reshape back to 28x28 for visualization

    # Display the size of the image array
    print(f"{title} - Image shape: {image.shape[1:]}")

    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5 * num_col, 2 * num_row))
    for i in range(num_row * num_col):
        ax = axes[i // num_col, i % num_col]
        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)
        ax.axis('off')

    plt.tight_layout()
    plt.show()

# Display training images (first 22 images from the training set)
X_train_batch = X_train[:22]  # Get the first 22 images from the training set
print("Training set images:")
show_images_with_size(X_train_batch, "Training Image", num_row=2, num_col=11)

# Display test images (first 22 images from the test set)
X_test_batch = X_test[:22]  # Get the first 22 images from the test set
print("Test set images:")
show_images_with_size(X_test_batch, "Test Image", num_row=2, num_col=11)

"""# Building the Neural Network Model using feedforward and backward propagation"""

import numpy as np
import time

class DeepNeuralNetwork():
    def __init__(self, sizes, activation='sigmoid', optimizer='momentum', init_strategy='he', dropout_rate=0.0):
        self.sizes = sizes
        self.optimizer = optimizer
        self.dropout_rate = dropout_rate

        # Activation function setup
        if activation == 'relu':
            self.activation = self.relu
        elif activation == 'sigmoid':
            self.activation = self.sigmoid
        elif activation == 'tanh':
            self.activation = self.tanh
        elif activation == 'leaky_relu':
            self.activation = self.leaky_relu
        else:
            raise ValueError("Activation function is currently not supported, please use 'relu', 'sigmoid', 'tanh', or 'leaky_relu'.")

        # Weight initialization setup
        self.init_strategy = init_strategy
        self.params = self.initialize()
        self.cache = {}

        # Initialize momentum_opt
        if self.optimizer == "momentum":
          self.momentum_opt = self.initialize_momentum_optimizer()

    def relu(self, x, derivative=False):
        if derivative:
            return np.where(x < 0, 0, 1)
        return np.maximum(0, x)

    def sigmoid(self, x, derivative=False):
        if derivative:
            return (np.exp(-x)) / ((np.exp(-x) + 1) ** 2)
        return 1 / (1 + np.exp(-x))

    def tanh(self, x, derivative=False):
        if derivative:
            return 1 - np.tanh(x) ** 2
        return np.tanh(x)

    def leaky_relu(self, x, derivative=False, alpha=0.01):
        if derivative:
            return np.where(x > 0, 1, alpha)
        return np.where(x > 0, x, alpha * x)

    def softmax(self, x):
        exps = np.exp(x - np.max(x))  # For numerical stability
        return exps / np.sum(exps, axis=0)

    def initialize(self):
        np.random.seed(57)  # Seed based on roll number (B22EE057 = 57)

        input_layer, hidden_layer, output_layer = self.sizes

        # Weight initialization strategies
        if self.init_strategy == 'he':
            return {
                "W1": np.random.randn(hidden_layer, input_layer) * np.sqrt(2. / input_layer),
                "b1": np.ones((hidden_layer, 1)),
                "W2": np.random.randn(output_layer, hidden_layer) * np.sqrt(2. / hidden_layer),
                "b2": np.ones((output_layer, 1))
            }
        elif self.init_strategy == 'xavier':
            return {
                "W1": np.random.randn(hidden_layer, input_layer) * np.sqrt(1. / input_layer),
                "b1": np.ones((hidden_layer, 1)),
                "W2": np.random.randn(output_layer, hidden_layer) * np.sqrt(1. / hidden_layer),
                "b2": np.ones((output_layer, 1))
            }
        elif self.init_strategy == 'lecun':
            return {
                "W1": np.random.randn(hidden_layer, input_layer) * np.sqrt(1. / input_layer),
                "b1": np.ones((hidden_layer, 1)),
                "W2": np.random.randn(output_layer, hidden_layer) * np.sqrt(1. / hidden_layer),
                "b2": np.ones((output_layer, 1))
            }
        else:
            raise ValueError("Initialization strategy not supported.")

    def initialize_momentum_optimizer(self):
        momentum_opt = {
            "W1": np.zeros(self.params["W1"].shape),
            "b1": np.zeros(self.params["b1"].shape),
            "W2": np.zeros(self.params["W2"].shape),
            "b2": np.zeros(self.params["b2"].shape),
        }
        return momentum_opt

    def feed_forward(self, x):
        self.cache["X"] = x
        self.cache["Z1"] = np.matmul(self.params["W1"], self.cache["X"].T) + self.params["b1"]
        self.cache["A1"] = self.activation(self.cache["Z1"])

        # Apply dropout after activation in hidden layers
        if self.dropout_rate > 0:
            self.cache["dropout_mask"] = np.random.rand(*self.cache["A1"].shape) < (1 - self.dropout_rate)
            self.cache["A1"] *= self.cache["dropout_mask"]

        self.cache["Z2"] = np.matmul(self.params["W2"], self.cache["A1"]) + self.params["b2"]
        self.cache["A2"] = self.softmax(self.cache["Z2"])
        return self.cache["A2"]

    def back_propagate(self, y, output):
        current_batch_size = y.shape[0]

        dZ2 = output - y.T
        dW2 = (1. / current_batch_size) * np.matmul(dZ2, self.cache["A1"].T)
        db2 = (1. / current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)

        dA1 = np.matmul(self.params["W2"].T, dZ2)
        dZ1 = dA1 * self.activation(self.cache["Z1"], derivative=True)
        dW1 = (1. / current_batch_size) * np.matmul(dZ1, self.cache["X"])
        db1 = (1. / current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)

        self.grads = {"W1": dW1, "b1": db1, "W2": dW2, "b2": db2}
        return self.grads

    def cross_entropy_loss(self, y, output):
        l_sum = np.sum(np.multiply(y.T, np.log(output)))
        m = y.shape[0]
        l = -(1. / m) * l_sum
        return l

    def optimize(self, l_rate=0.1, beta=.9):
        if self.optimizer == "sgd":
            for key in self.params:
                self.params[key] = self.params[key] - l_rate * self.grads[key]
        elif self.optimizer == "momentum":
            for key in self.params:
                self.momentum_opt[key] = (beta * self.momentum_opt[key] + (1. - beta) * self.grads[key])
                self.params[key] = self.params[key] - l_rate * self.momentum_opt[key]
        else:
            raise ValueError("Optimizer not supported, please use 'sgd' or 'momentum'.")

    def accuracy(self, y, output):
        return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))

    def early_stopping(self, train_loss, test_loss, best_test_loss, patience, epoch, best_epoch):
        if test_loss < best_test_loss:
            best_test_loss = test_loss
            best_epoch = epoch
        elif epoch - best_epoch > patience:
            print(f"Early stopping triggered at epoch {epoch}")
            return True, best_test_loss, best_epoch
        return False, best_test_loss, best_epoch

    def train(self, x_train, y_train, x_test, y_test, epochs=25, batch_size=22, optimizer='momentum',
              l_rate=0.1, beta=.9, early_stopping_patience=5):
        self.epochs = epochs
        self.batch_size = batch_size
        num_batches = -(-x_train.shape[0] // self.batch_size)

        # Initialize optimizer
        self.optimizer = optimizer
        if self.optimizer == 'momentum':
            self.momemtum_opt = self.initialize_momentum_optimizer()

        # Early stopping variables
        best_test_loss = float('inf')
        best_epoch = 0

        start_time = time.time()
        template = "Epoch {}: {:.2f}s, train acc={:.2f}, train loss={:.2f}, test acc={:.2f}, test loss={:.2f}"

        for epoch in range(self.epochs):
            # Shuffle
            permutation = np.random.permutation(x_train.shape[0])
            x_train_shuffled = x_train[permutation]
            y_train_shuffled = y_train[permutation]

            for j in range(num_batches):
                begin = j * self.batch_size
                end = min(begin + self.batch_size, x_train.shape[0]-1)
                x = x_train_shuffled[begin:end]
                y = y_train_shuffled[begin:end]

                # Forward and Backprop
                output = self.feed_forward(x)
                grad = self.back_propagate(y, output)
                self.optimize(l_rate=l_rate, beta=beta)

            # Evaluate performance after each epoch
            output = self.feed_forward(x_train)
            train_acc = self.accuracy(y_train, output)
            train_loss = self.cross_entropy_loss(y_train, output)
            output = self.feed_forward(x_test)
            test_acc = self.accuracy(y_test, output)
            test_loss = self.cross_entropy_loss(y_test, output)

            print(template.format(epoch + 1, time.time() - start_time, train_acc, train_loss, test_acc, test_loss))

            # Early stopping check
            stop, best_test_loss, best_epoch = self.early_stopping(train_loss, test_loss, best_test_loss, early_stopping_patience, epoch, best_epoch)
            if stop:
                break

"""## Results"""

from sklearn.model_selection import train_test_split

"""# Plotting the accuracy and loss per epoch"""

def train_and_evaluate_with_plots(X, y, splits, epochs=25, batch_size=22, optimizer='momentum', l_rate=0.1, beta=0.9):
    """
    Train and evaluate the Deep Neural Network on different train-test splits and plot accuracy and loss.

    Parameters:
        X_train (np.array): Input data.
        y_train (np.array): One-hot encoded labels.
        splits (list): List of train-test split ratios (e.g., [0.7, 0.8, 0.9]).
        epochs (int): Number of epochs to train.
        batch_size (int): Batch size for training.
        optimizer (str): Optimizer to use ('momentum' or 'sgd').
        l_rate (float): Learning rate.
        beta (float): Momentum parameter.
    """
    for split_ratio in splits:
        # Split the dataset
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-split_ratio, random_state=42)

        # Initialize the DNN
        dnn = DeepNeuralNetwork(sizes=[X.shape[1], 64, y.shape[1]], activation='sigmoid', optimizer=optimizer)

        # Metrics storage
        train_accuracies = []
        train_losses = []
        test_accuracies = []
        test_losses = []

        # Train the model
        print(f"Training with {int(split_ratio * 100)}:{int((1 - split_ratio) * 100)} Train-Test Split")
        for epoch in range(epochs):
            # Shuffle training data once
            permutation = np.random.permutation(X_train.shape[0])
            X_train_shuffled = X_train[permutation]
            y_train_shuffled = y_train[permutation]

            num_batches = -(-X_train.shape[0] // batch_size)
            for batch_idx in range(num_batches):
                begin = batch_idx * batch_size
                end = min(begin + batch_size, X_train.shape[0])
                x_batch = X_train_shuffled[begin:end]
                y_batch = y_train_shuffled[begin:end]

                # Train on batch
                dnn.feed_forward(x_batch)
                dnn.back_propagate(y_batch, dnn.cache["A2"])
                dnn.optimize(l_rate=l_rate, beta=beta)

            # Evaluate on the entire training and test sets
            train_output = dnn.feed_forward(X_train)
            train_acc = dnn.accuracy(y_train, train_output)
            train_loss = dnn.cross_entropy_loss(y_train, train_output)
            test_output = dnn.feed_forward(X_test)
            test_acc = dnn.accuracy(y_test, test_output)
            test_loss = dnn.cross_entropy_loss(y_test, test_output)

            train_accuracies.append(train_acc)
            train_losses.append(train_loss)
            test_accuracies.append(test_acc)
            test_losses.append(test_loss)

            print(f"Epoch {epoch+1}/{epochs}: Train Acc={train_acc:.4f}, Train Loss={train_loss:.4f}, "
                  f"Test Acc={test_acc:.4f}, Test Loss={test_loss:.4f}")

        # Plot the metrics
        epochs_range = range(1, epochs + 1)
        plt.figure(figsize=(12, 6))

        # Accuracy plot
        plt.subplot(1, 2, 1)
        plt.plot(epochs_range, train_accuracies, label='Train Accuracy')
        plt.plot(epochs_range, test_accuracies, label='Test Accuracy')
        plt.title(f'Accuracy for {int(split_ratio * 100)}:{int((1 - split_ratio) * 100)} Split')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.legend()

        # Loss plot
        plt.subplot(1, 2, 2)
        plt.plot(epochs_range, train_losses, label='Train Loss')
        plt.plot(epochs_range, test_losses, label='Test Loss')
        plt.title(f'Loss for {int(split_ratio * 100)}:{int((1 - split_ratio) * 100)} Split')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()

        plt.tight_layout()
        plt.show()

splits = [0.7, 0.8, 0.9]  # Train-test split ratios
train_and_evaluate_with_plots(X_train, y_train, splits, epochs=25, batch_size=22, optimizer='momentum', l_rate=0.1, beta=0.9)

"""# Confusion matrix for all the combinations of the network"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def train_evaluate_and_confusion(X, y, splits, epochs=25, batch_size=22, optimizer='momentum', l_rate=0.1, beta=0.9):
    """
    Train the Deep Neural Network on different train-test splits and display confusion matrices.

    Parameters:
        X_train (np.array): Input data.
        y_train (np.array): One-hot encoded labels.
        splits (list): List of train-test split ratios (e.g., [0.7, 0.8, 0.9]).
        epochs (int): Number of epochs to train.
        batch_size (int): Batch size for training.
        optimizer (str): Optimizer to use ('momentum' or 'sgd').
        l_rate (float): Learning rate.
        beta (float): Momentum parameter.
    """
    for split_ratio in splits:
        # Split the dataset
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-split_ratio, random_state=42)

        # Initialize the DNN
        dnn = DeepNeuralNetwork(sizes=[X.shape[1], 64, y.shape[1]], activation='sigmoid', optimizer=optimizer)

        # Initialize momentum optimizer if required
        if optimizer == 'momentum':
            dnn.momemtum_opt = dnn.initialize_momentum_optimizer()

        # Train the model
        print(f"Training with {int(split_ratio * 100)}:{int((1 - split_ratio) * 100)} Train-Test Split")
        for epoch in range(epochs):
            # Shuffle training data
            permutation = np.random.permutation(X_train.shape[0])
            X_train_shuffled = X_train[permutation]
            y_train_shuffled = y_train[permutation]

            num_batches = -(-X_train.shape[0] // batch_size)
            for batch_idx in range(num_batches):
                begin = batch_idx * batch_size
                end = min(begin + batch_size, X_train.shape[0])
                x_batch = X_train_shuffled[begin:end]
                y_batch = y_train_shuffled[begin:end]

                # Train on batch
                dnn.feed_forward(x_batch)
                dnn.back_propagate(y_batch, dnn.cache["A2"])
                dnn.optimize(l_rate=l_rate, beta=beta)

        # Test the model and generate confusion matrix
        test_output = dnn.feed_forward(X_test)
        y_pred = np.argmax(test_output.T, axis=-1)
        y_true = np.argmax(y_test, axis=-1)

        # Generate and display the confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        print(f"Confusion Matrix for {int(split_ratio * 100)}:{int((1 - split_ratio) * 100)} Train-Test Split")
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(y.shape[1]))
        disp.plot(cmap=plt.cm.Blues)
        plt.title(f"Confusion Matrix ({int(split_ratio * 100)}:{int((1 - split_ratio) * 100)} Split)")
        plt.show()

splits = [0.7, 0.8, 0.9]  # Train-test split ratios
train_evaluate_and_confusion(X_train, y_train, splits, epochs=25, batch_size=22, optimizer='momentum', l_rate=0.1, beta=0.9)

"""# Code to Calculate Parameters"""

def report_parameters(dnn):
    """
    Report the total trainable and non-trainable parameters in the model.

    Parameters:
        dnn (DeepNeuralNetwork): An instance of the DeepNeuralNetwork class.

    Returns:
        None
    """
    trainable_params = 0
    for key, value in dnn.params.items():
        trainable_params += np.prod(value.shape)

    # Assuming no non-trainable parameters in this implementation
    non_trainable_params = 0

    print(f"Total Trainable Parameters: {trainable_params}")
    print(f"Total Non-Trainable Parameters: {non_trainable_params}")

# Example usage
dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='sigmoid', optimizer='momentum')
report_parameters(dnn)

